{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "path_main_folder = '/home/antorosi/Documents/Prediction'\n",
    "sys.path.append(path_main_folder)\n",
    "\n",
    "from conso.load_shape_data import load_data_conso, get_uniformed_data_conso, change_granularity, get_x_y_prediction_conso, get_train_test_sets, normalized_dataset, select_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and shape data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "path_data = os.path.join(path_main_folder, 'data')\n",
    "dict_data_conso = load_data_conso(path_data)\n",
    "\n",
    "# Uniformization\n",
    "data_conso_df, dict_colnames_conso = get_uniformed_data_conso(dict_data_conso)\n",
    "\n",
    "# Granularity from 15 min to 1H\n",
    "data_conso_df = change_granularity(data_conso_df, granularity=\"1H\")\n",
    "\n",
    "# Get x and y from prediction\n",
    "x_conso, y_conso, dict_colnames_conso = get_x_y_prediction_conso(data_conso_df, dict_colnames_conso, lags=[24,48])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder to store results\n",
    "path_out = os.path.join(path_main_folder, 'out', 'cv_cmca_model_rf_0')\n",
    "if not os.path.exists(path_out):\n",
    "    os.mkdir(path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables used for input\n",
    "selected_variables = ['conso', 'calendar', 'meteo']\n",
    "gen_name = 'cmca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test periods for each K step of the cross-validation\n",
    "cv_periods = {}\n",
    "cv_periods['period_1'] = (datetime.datetime(year=2013, month=1, day=1), datetime.datetime(year=2013, month=12, day=31))\n",
    "cv_periods['period_2'] = (datetime.datetime(year=2014, month=1, day=1), datetime.datetime(year=2014, month=12, day=31))\n",
    "cv_periods['period_3'] = (datetime.datetime(year=2015, month=1, day=1), datetime.datetime(year=2015, month=12, day=31))\n",
    "cv_periods['period_4'] = (datetime.datetime(year=2016, month=1, day=1), datetime.datetime(year=2016, month=12, day=31))\n",
    "cv_periods['period_5'] = (datetime.datetime(year=2017, month=1, day=1), datetime.datetime(year=2017, month=12, day=31))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting each datasets\n",
    "dict_datasets = {}\n",
    "for key, date_period in cv_periods.items():\n",
    "    x_conso_selected_var = select_variables(x_conso, dict_colnames_conso, selected_variables)\n",
    "    dataset, dict_ds = get_train_test_sets(x_conso_selected_var, y_conso, date_period[0], date_period[1])\n",
    "    dataset = normalized_dataset(dataset, dict_colnames_conso)\n",
    "    \n",
    "    dict_datasets[key] = {'dataset': dataset, 'dict_ds': dict_ds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results wrap up \n",
    "results_df = pd.DataFrame(columns=['name', 'train_mse',\n",
    "                                           'train_mae', 'train_mape',\n",
    "                                           'test_mse', 'test_mae',\n",
    "                                           'test_mape'])\n",
    "path_results = path_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Model 1 =========================\n",
      "========================= Model 2 =========================\n",
      "========================= Model 3 =========================\n",
      "========================= Model 4 =========================\n",
      "========================= Model 5 =========================\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "for name_period, el in dict_datasets.items():\n",
    "    dataset = el['dataset']\n",
    "\n",
    "    print('========================= Model {} ========================='.format(idx+1))\n",
    "\n",
    "    # Prepare model characteristics and folders\n",
    "    name_model = '{}_RF_{}_norm'.format(name_period, gen_name)\n",
    "\n",
    "    path_model = os.path.join(path_out, name_model)\n",
    "    if not os.path.exists(path_model):\n",
    "        os.mkdir(path_model)\n",
    "    \n",
    "    # Compile model\n",
    "    model = linear_model.LinearRegression(n_jobs=-1)\n",
    "    model = RandomForestRegressor(random_state=0, n_estimators=350, max_depth=50, min_samples_leaf=2,\n",
    "                                  n_jobs=-1, max_features=1/3)\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X=dataset['train']['x'], y=np.ravel(dataset['train']['y']))\n",
    "    \n",
    "    # Save model\n",
    "    with open(os.path.join(path_model,'model.pickle'),'wb') as f:\n",
    "        pickle.dump(model,f)\n",
    "\n",
    "    # Get results\n",
    "    y_train = np.ravel(dataset['train']['y'])\n",
    "    y_hat_train = model.predict(dataset['train']['x'])\n",
    "    y_test= np.ravel(dataset['test']['y'])\n",
    "    y_hat_test = model.predict(dataset['test']['x'])\n",
    "    \n",
    "    result = {}\n",
    "    result['name'] = name_model\n",
    "    \n",
    "    result['train_mse'] = mean_squared_error(y_train, y_hat_train)\n",
    "    result['train_mae'] = mean_absolute_error(y_train, y_hat_train)\n",
    "    result['train_mape'] = mean_absolute_percentage_error(y_train, y_hat_train)\n",
    "    result['test_mse'] = mean_squared_error(y_test, y_hat_test)\n",
    "    result['test_mae'] = mean_absolute_error(y_test, y_hat_test)\n",
    "    result['test_mape'] = mean_absolute_percentage_error(y_test, y_hat_test)\n",
    "\n",
    "    # Append result to results_df\n",
    "    \n",
    "    results_df= results_df.append(result, ignore_index=True)\n",
    "    results_df.to_csv(os.path.join(path_results, 'cv_results.csv'), sep=';')\n",
    "\n",
    "    idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_out, 'dict_datasets.pickle'),'wb') as f:\n",
    "    pickle.dump(dict_datasets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_period = [datetime.datetime(2013,1,1), datetime.datetime(2017,12,31)]\n",
    "\n",
    "x_conso_selected_var = select_variables(x_conso, dict_colnames_conso, selected_variables)\n",
    "dataset, dict_ds = get_train_test_sets(x_conso_selected_var, y_conso, date_period[0], date_period[1])\n",
    "dataset = normalized_dataset(dataset, dict_colnames_conso)\n",
    "\n",
    "dataset = dataset['test']\n",
    "dict_ds=dict_ds['test']\n",
    "\n",
    "x = dataset['x']\n",
    "y = dataset['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = list()\n",
    "test_indices = list()\n",
    "\n",
    "for year in [2013,2014,2015,2016,2017]:\n",
    "    mask = dict_ds.dt.year == year\n",
    "    test_indices.append(np.where(mask)[0])\n",
    "    train_indices.append(np.where(np.invert(mask))[0])\n",
    "\n",
    "custom_cv = zip(train_indices, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 400, num = 8)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['sqrt', 'auto']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4, 6, 8, 10, 15, 20]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] min_samples_leaf=2, min_samples_split=5, n_estimators=200, max_depth=100, max_features=sqrt \n",
      "[CV] min_samples_leaf=2, min_samples_split=5, n_estimators=200, max_depth=100, max_features=sqrt \n",
      "[CV] min_samples_leaf=2, min_samples_split=5, n_estimators=200, max_depth=100, max_features=sqrt \n",
      "[CV] min_samples_leaf=2, min_samples_split=5, n_estimators=200, max_depth=100, max_features=sqrt \n",
      "[CV] min_samples_leaf=2, min_samples_split=5, n_estimators=200, max_depth=100, max_features=sqrt \n",
      "[CV]  min_samples_leaf=2, min_samples_split=5, n_estimators=200, max_depth=100, max_features=sqrt, total=  54.2s\n",
      "[CV]  min_samples_leaf=2, min_samples_split=5, n_estimators=200, max_depth=100, max_features=sqrt, total=  55.2s\n",
      "[CV]  min_samples_leaf=2, min_samples_split=5, n_estimators=200, max_depth=100, max_features=sqrt, total=  55.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   3 out of   5 | elapsed:   58.2s remaining:   38.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  min_samples_leaf=2, min_samples_split=5, n_estimators=200, max_depth=100, max_features=sqrt, total= 1.1min\n",
      "[CV]  min_samples_leaf=2, min_samples_split=5, n_estimators=200, max_depth=100, max_features=sqrt, total= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   5 out of   5 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=<zip object at 0x7f1918535f48>, error_score='raise',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=1, n_jobs=6,\n",
       "          param_distributions={'min_samples_leaf': [1, 2, 4, 6, 8, 10, 15, 20], 'min_samples_split': [2, 5, 10], 'n_estimators': [50, 100, 150, 200, 250, 300, 350, 400], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110], 'max_features': ['sqrt', 'auto']},\n",
       "          pre_dispatch='2*n_jobs', random_state=44, refit=True,\n",
       "          return_train_score='warn', scoring='neg_mean_squared_error',\n",
       "          verbose=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(scoring='neg_mean_squared_error', estimator = rf, param_distributions = random_grid, n_iter = 1, cv = custom_cv, verbose=2, random_state=44, n_jobs = 6)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X=x, y=np.ravel(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_out,'rf_random.pickle'), 'wb') as f:\n",
    "    pickle.dump(rf_random,f)\n",
    "\n",
    "with open(os.path.join(path_out,'rf_random.pickle'), 'rb') as f:\n",
    "    rf_random1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_out,'results.pickle'), 'wb') as f:\n",
    "    pickle.dump(rf_random.cv_results_,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_out,'results.pickle'), 'rb') as f:\n",
    "    results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/antorosi/PycharmProjects/KERAS-TS-VENV/lib/python3.5/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/local/home/antorosi/PycharmProjects/KERAS-TS-VENV/lib/python3.5/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/local/home/antorosi/PycharmProjects/KERAS-TS-VENV/lib/python3.5/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/local/home/antorosi/PycharmProjects/KERAS-TS-VENV/lib/python3.5/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/local/home/antorosi/PycharmProjects/KERAS-TS-VENV/lib/python3.5/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/local/home/antorosi/PycharmProjects/KERAS-TS-VENV/lib/python3.5/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/local/home/antorosi/PycharmProjects/KERAS-TS-VENV/lib/python3.5/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([59.80345397]),\n",
       " 'mean_score_time': array([0.61167545]),\n",
       " 'mean_test_score': array([-8018984.22173199]),\n",
       " 'mean_train_score': array([-818715.59532868]),\n",
       " 'param_max_depth': masked_array(data=[100],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_features': masked_array(data=['sqrt'],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_leaf': masked_array(data=[2],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_split': masked_array(data=[5],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[200],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 100,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200}],\n",
       " 'rank_test_score': array([1], dtype=int32),\n",
       " 'split0_test_score': array([-8767884.49943025]),\n",
       " 'split0_train_score': array([-801343.32908668]),\n",
       " 'split1_test_score': array([-7478107.01852783]),\n",
       " 'split1_train_score': array([-821569.2786664]),\n",
       " 'split2_test_score': array([-8403510.03887971]),\n",
       " 'split2_train_score': array([-813468.99169389]),\n",
       " 'split3_test_score': array([-7495938.56537713]),\n",
       " 'split3_train_score': array([-839083.37010551]),\n",
       " 'split4_test_score': array([-7950538.94563452]),\n",
       " 'split4_train_score': array([-818113.00709092]),\n",
       " 'std_fit_time': array([6.77890135]),\n",
       " 'std_score_time': array([0.11041896]),\n",
       " 'std_test_score': array([506151.58553563]),\n",
       " 'std_train_score': array([12269.27344727])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf_random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ea266b0743c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrf_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'rf_random' is not defined"
     ]
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9790588560688929"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(x,np.ravel(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rf.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35108,)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35108,)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35108, 1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35108,)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.reshape(-1,1).flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
